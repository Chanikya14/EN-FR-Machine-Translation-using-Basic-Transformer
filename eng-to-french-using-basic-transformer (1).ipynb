{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-20T07:02:25.429547Z","iopub.execute_input":"2024-12-20T07:02:25.430161Z","iopub.status.idle":"2024-12-20T07:02:33.545226Z","shell.execute_reply.started":"2024-12-20T07:02:25.430098Z","shell.execute_reply":"2024-12-20T07:02:33.543977Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install sacremoses\n# Load tokenizer for French-to-English translation\ntokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-fr-en\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T07:02:33.547229Z","iopub.execute_input":"2024-12-20T07:02:33.547878Z","iopub.status.idle":"2024-12-20T07:02:49.037138Z","shell.execute_reply.started":"2024-12-20T07:02:33.547838Z","shell.execute_reply":"2024-12-20T07:02:49.035863Z"}},"outputs":[{"name":"stdout","text":"Collecting sacremoses\n  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from sacremoses) (2024.5.15)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from sacremoses) (8.1.7)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from sacremoses) (1.4.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sacremoses) (4.66.4)\nDownloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: sacremoses\nSuccessfully installed sacremoses-0.1.1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d582a22e76a494c87502b7351ba8251"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.42k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"582033ff7ac64e629f5ece7cb460b04c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"source.spm:   0%|          | 0.00/802k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"872029de6a894f4e9cc4ef7abbda2f59"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"target.spm:   0%|          | 0.00/778k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9d0646bd69b493aa73f8cfeaa8802cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.34M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79b0325c6d52430fa7b9777ad2461db8"}},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"dataset = load_dataset(\"wmt14\", \"fr-en\")\n\n# Inspect the dataset\nprint(dataset)\n\n# Access the train and test sets\ntrain_data_orig = dataset['train']\ntest_data_orig = dataset['test']\n\n# Example of a sample\nprint(train_data_orig[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T07:02:49.038619Z","iopub.execute_input":"2024-12-20T07:02:49.038988Z","iopub.status.idle":"2024-12-20T07:07:19.657346Z","shell.execute_reply.started":"2024-12-20T07:02:49.038923Z","shell.execute_reply":"2024-12-20T07:07:19.654681Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3628605b4c6e4ad59d6932541bfbb0e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d355b1bc7b3481f9b20dc0278774918"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0/30 [00:00<?, ?files/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6bb56b9ee6984f2aaec512fc15141a5d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00030.parquet:   0%|          | 0.00/252M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a9997d179c24115bf59ff1f7550c177"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00001-of-00030.parquet:   0%|          | 0.00/241M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26ef5df801fc4073bb9f254dcec682ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00002-of-00030.parquet:   0%|          | 0.00/243M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f87212858874f078a55e3c22368e2b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00003-of-00030.parquet:   0%|          | 0.00/247M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37f166a111a649faa867398ef7f838ea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00004-of-00030.parquet:   0%|          | 0.00/242M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a93c672630e24c14b43301f28b1182d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00005-of-00030.parquet:   0%|          | 0.00/238M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8aa372520205493f81073534381ba548"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00006-of-00030.parquet:   0%|          | 0.00/240M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d6a2d8cfb634758aa09b599c3e3265c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00007-of-00030.parquet:   0%|          | 0.00/241M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94078f0be83342f5b217ca9f73f6bffd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00008-of-00030.parquet:   0%|          | 0.00/242M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55739a7370e545c6959ff613ff252f11"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00009-of-00030.parquet:   0%|          | 0.00/239M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bbbfeb6cfbfd4fde8a27e68c5a66a084"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00010-of-00030.parquet:   0%|          | 0.00/239M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d755bdcebcc1408994d75a1a0a03b78b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00011-of-00030.parquet:   0%|          | 0.00/241M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e69b37d810bb4d46a07ec663cb374038"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00012-of-00030.parquet:   0%|          | 0.00/241M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8132fac06b84d87be955c795abb7900"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00013-of-00030.parquet:   0%|          | 0.00/230M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd7d4e5acc4b4319b5707a28c700f38d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00014-of-00030.parquet:   0%|          | 0.00/214M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"815fd1fd834e4d3d8b8ce7328dc60000"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00015-of-00030.parquet:   0%|          | 0.00/231M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b4939dfac9948e3808e9ebe351e55cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00016-of-00030.parquet:   0%|          | 0.00/227M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20d29b80e7d14114a24dd14c5de5479b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00017-of-00030.parquet:   0%|          | 0.00/226M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff9fc930fc5b4e878a80214ebe9fde72"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00018-of-00030.parquet:   0%|          | 0.00/261M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55c1ff0c8f3544b78238582b5ecd8180"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00019-of-00030.parquet:   0%|          | 0.00/259M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94441595d9e04623b5504ec588e2678b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00020-of-00030.parquet:   0%|          | 0.00/261M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23939852039240709462e8a785cf6c6c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00021-of-00030.parquet:   0%|          | 0.00/264M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebc4a6f70a544b96a1a5075ed08059c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00022-of-00030.parquet:   0%|          | 0.00/267M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ee387967bb04b5aa7653d1ec8755249"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00023-of-00030.parquet:   0%|          | 0.00/270M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ce28303bd434ddc8ca183483dfd275a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00024-of-00030.parquet:   0%|          | 0.00/274M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a758a1e186d5474ca3e79e8cbea0f2dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00025-of-00030.parquet:   0%|          | 0.00/278M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6cae2fd86134ec3bf0dfa426e38204b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00026-of-00030.parquet:   0%|          | 0.00/365M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9daac5495bd34b5aa80fa5fc35ff806f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00027-of-00030.parquet:   0%|          | 0.00/322M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6c0d8f21abf400c9799ee5da64bd401"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00028-of-00030.parquet:   0%|          | 0.00/370M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7872bed0b6142b4bb92e7de0727d3c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00029-of-00030.parquet:   0%|          | 0.00/311M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"822c6dcefd2d452995847a20ac8cdef9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/475k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"557b6a20a4874b9298dc2f78f9ea5e21"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/536k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"914c7208c3f74b98854e65de298a7fb5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/40836715 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"557e4ded76eb4221b58e8add831420ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/3000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19899d5d57734502b08d7b47470cc0e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/3003 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b86d3e45682477d911ba9625929e912"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading dataset shards:   0%|          | 0/30 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"778760369f924965bb0579f9d95da11f"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['translation'],\n        num_rows: 40836715\n    })\n    validation: Dataset({\n        features: ['translation'],\n        num_rows: 3000\n    })\n    test: Dataset({\n        features: ['translation'],\n        num_rows: 3003\n    })\n})\n{'translation': {'en': 'Resumption of the session', 'fr': 'Reprise de la session'}}\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Sampling the data since it;s huge!!\nfraction = 0.0005\n\n\n# Reduce the train and test datasets\ntrain_data = train_data_orig.select(range(int(len(train_data_orig) * fraction)))\ntest_data = test_data_orig.select(range(int(len(test_data_orig) * fraction)))\n\nprint(len(train_data))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T07:07:19.662644Z","iopub.execute_input":"2024-12-20T07:07:19.663464Z","iopub.status.idle":"2024-12-20T07:07:19.683106Z","shell.execute_reply.started":"2024-12-20T07:07:19.663401Z","shell.execute_reply":"2024-12-20T07:07:19.681096Z"}},"outputs":[{"name":"stdout","text":"20418\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"input_data = []\noutput_data = []\nfor data in train_data['translation']:\n    input_data.append(data['en'])\n    output_data.append(data['fr'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T07:07:19.684815Z","iopub.execute_input":"2024-12-20T07:07:19.685269Z","iopub.status.idle":"2024-12-20T07:07:19.888081Z","shell.execute_reply.started":"2024-12-20T07:07:19.685232Z","shell.execute_reply":"2024-12-20T07:07:19.886632Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"input_tokens = []\noutput_tokens = []\n\nfor input in input_data:\n    input_tokens.append(tokenizer(input, return_tensors=\"pt\"))\n\nprint(\"done with input\")\nfor output in output_data:\n    output_tokens.append(tokenizer(output, return_tensors=\"pt\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T07:07:19.889840Z","iopub.execute_input":"2024-12-20T07:07:19.890221Z","iopub.status.idle":"2024-12-20T07:07:33.511086Z","shell.execute_reply.started":"2024-12-20T07:07:19.890187Z","shell.execute_reply":"2024-12-20T07:07:33.509537Z"}},"outputs":[{"name":"stdout","text":"done with input\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"input_tokens[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T07:07:33.513097Z","iopub.execute_input":"2024-12-20T07:07:33.513600Z","iopub.status.idle":"2024-12-20T07:07:33.565359Z","shell.execute_reply.started":"2024-12-20T07:07:33.513525Z","shell.execute_reply":"2024-12-20T07:07:33.564072Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"{'input_ids': tensor([[19784, 23484,  3498,     7,     4,   269,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport math","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T07:07:33.567125Z","iopub.execute_input":"2024-12-20T07:07:33.567670Z","iopub.status.idle":"2024-12-20T07:07:33.574529Z","shell.execute_reply.started":"2024-12-20T07:07:33.567613Z","shell.execute_reply":"2024-12-20T07:07:33.572938Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# For generating token embeddings by extending nn.Embedding class\nclass TokenEmbeddings(nn.Embedding):\n    def __init__(self, vocab_size, d_model):\n        super().__init__(num_embeddings=vocab_size, embedding_dim=d_model, padding_idx=0)\n\n# For generating positional embeddings since attention inherently doesn't know positions\nclass PositionalEmbeddings(nn.Module):\n    def __init__(self, d_model, max_len, device):\n        super().__init__()\n        self.encoding = torch.zeros(max_len, d_model, device=device)\n        self.encoding.requires_grad = False  # No gradients for fixed positional encodings\n\n        # Compute position indices and frequency indices\n        positions = torch.arange(0, max_len, device=device).unsqueeze(1)  # Shape: (max_len, 1)\n        indices = torch.arange(0, d_model, device=device).unsqueeze(0)    # Shape: (1, d_model)\n        \n        # Compute frequency terms\n        div_term = torch.pow(10000.0, (2 * (indices // 2)) / d_model)  # Shape: (1, d_model)\n\n        # Apply sine to even indices and cosine to odd indices\n        self.encoding[:, 0::2] = torch.sin(positions / div_term[:, 0::2])  # Even indices\n        self.encoding[:, 1::2] = torch.cos(positions / div_term[:, 1::2])  # Odd indices\n\n    def forward(self, x):\n        batch_sz, seq_len = x.size()\n        return self.encoding[:seq_len, :]\n\n# Final embeddings\nclass TransformerEmbeddings(nn.Module):\n    def __init__(self, vocab_size, d_model, max_len, device):\n        super().__init__()\n        self.token_emb = TokenEmbeddings(vocab_size, d_model)\n        self.pos_emb = PositionalEmbeddings(d_model, max_len, device)\n    def forward(x):\n        tok = self.tok_emb(x)\n        pos = self.pos_emb(x)\n        return tok+pos","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T07:07:33.576292Z","iopub.execute_input":"2024-12-20T07:07:33.576714Z","iopub.status.idle":"2024-12-20T07:07:33.593608Z","shell.execute_reply.started":"2024-12-20T07:07:33.576676Z","shell.execute_reply":"2024-12-20T07:07:33.592164Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Basic attention class\nclass BasicAttention(nn.Module):\n    def __init__(self, queries, keys, values, d_model, mask=None):\n        super().__init__()\n        self.queries = queries\n        self.keys = keys\n        self.values = values\n        self.d_model = d_model\n        self.softmax = nn.Softmax(dim=-1)\n        \n    def forward():\n        # each is of dim batch_sz*d_k(or d_model)\n        cares = torch.matmul(self.queries, self.keys.transpose(-2, -1))\n        cares_scaled = cares*(1/math.sqrt(d_model))\n        if mask is not None:\n            cares_scaled = cares_scaled.masked_fill(mask==0, -100000)\n        cares_probs = self.softmax(cares_scaled)\n        attention = torch.matmul(cares_probs, self.values)\n        return attention\n        \n# Multi-head attention\nclass MultiHeadAttention(nn.Module):\n    def __init(self, d_model, num_heads, dk, dv):\n        super().__init__()\n        self.embed_size = d_model\n        self.n_heads = num_heads\n        assert d_model%num_heads == 0\n        self.head_dim = d_model/num_heads\n\n        self.queries = nn.Linear(d_model, dk)\n        self.keys = nn.Linear(d_model, dk)\n        self.values = nn.Linear(d_model, dv)\n        self.out = nn.Linear(num_heads*dv, d_model)\n\n    def split(self, _tmp):\n        # Funtion to split into n heads\n        batch_size = _tmp.shape[0]\n        seq_len = _tmp.shape[1]\n        _tmp = _tmp.reshape(batch_size, seq_len, self.n_heads, self.head_dim)\n        return _tmp.transpose(1, 2)\n\n    def concat(self, _tmp):\n        # Function to concat from heads\n        # input dim = [batch_sz, n_heads, seq_len, head_dim]\n        # out dim = [batch_sz, seq_len, d_model]\n        \n        batch_size = _tmp.shape[0]\n        seq_len = _tmp.shape[2]\n        _tmp = _tmp.transpose(1, 2).reshape(batch_size, seq_len, self.n_heads*self.head_dim)\n        return _tmp\n\n    def forward(self, queries, keys, values, mask=None):\n        queries_t = self.queries(queries)\n        keys_t = self.keys(keys)\n        values_t = self.values(values)\n\n        # Need to split into heads\n        queries_split = self.split(queries_t)\n        keys_split = self.split(keys_t)\n        values_split = self.split(values_t)\n\n        basic = BasicAttention(queries_split, keys_split, values_split, mask=mask)\n        attention = basic()\n\n        conc = self.concat(attention)\n        out = self.out(conc)\n        return out\n# Position-wise Feed-Forward Networks\nclass PositionwiseFFN(nn.Module):\n    def __init__(self, d_model, d_hidden, drop_prob):\n        super().__init__()\n        self.linear1 = nn.Linear(d_model, d_hidden)\n        self.linear2 = nn.Linear(d_hidden, d_model)\n        self.relu = nn.ReLU()\n        self.drop = nn.Dropout(p=drop_prob)\n    def forward(x):\n        x = self.linear1(x)\n        x = relu(x)\n        x = self.drop(x)\n        x = self.linear2(x)\n        return x\n# Layer Norm\nclass LayerNorm(nn.Module):\n    def __init__(self, d_model, eps=1e-5):\n        super().__init__()\n        self.layerNorm = nn.LayerNorm(d_model, eps=eps)\n    def forward(self, x):\n        return self.layerNorm(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T07:26:14.387789Z","iopub.execute_input":"2024-12-20T07:26:14.388243Z","iopub.status.idle":"2024-12-20T07:26:14.407466Z","shell.execute_reply.started":"2024-12-20T07:26:14.388208Z","shell.execute_reply":"2024-12-20T07:26:14.405546Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# Encoder layer\nclass EncoderLayer(nn.Module):\n    def __init__(self, d_model, d_hidden, n_heads, dk, dv, p_drop):\n        super().__init__()\n        self.mha = MultiHeadAttention(d_model, n_heads, dk, dv)\n        self.lnorm1 = LayerNorm(d_model)\n        self.drop1 = nn.Dropout(p=p_drop)\n        \n        self.ffn = PositionwiseFFN(d_model, d_hidden, p_drop)\n        self.lnorm2 = LayerNorm(d_model)\n        self.drop2 = nn.Dropout(p=p_drop)\n        \n    def forward(self, x, mask):\n        att = self.mha(x, x, x, mask)\n        drop_att = self.drop1(att)\n        norm_att = self.lnorm1(x+drop_att)\n\n        fout = self.ffn(norm_att)\n        drop_ffn = self.drop2(fout)\n        norm_ffn = self.lnorm1(norm_att+drop_ffn)\n\n        return norm_ffn\n\n# Decoder layer\nclass DecoderLayer(nn.Module):\n    def __init__(self, d_model, d_hidden, n_heads, dk, dv, p_drop):\n        super().__init__()\n        self.mha = MultiHeadAttention(d_model, n_heads, dk, dv)\n        self.lnorm1 = LayerNorm(d_model)\n        self.drop1 = nn.Dropout(p=p_drop)\n\n        self.mha2 = MultiHeadAttention(d_model, n_heads, dk, dv)\n        self.lnorm2 = LayerNorm(d_model)\n        self.drop2 = nn.Dropout(p=p_drop)\n\n        self.ffn = PositionwiseFFN(d_model, d_hidden, p_drop)\n        self.lnorm3 = LayerNorm(d_model)\n        self.drop3 = nn.Dropout(p=p_drop)\n\n    def forward(self, dec, enc, out_mask, in_mask):\n        att = self.mha(dec, dec, dec, out_mask)\n        drop_att = self.drop1(att)\n        norm_att = self.lnorm1(dec+drop_att)\n        prev = norm_att\n\n        if enc is not None:\n            att2 = self.mha2(norm_att, enc, enc, in_mask)\n            drop_att2 = self.drop2(att2)\n            norm_att2 = self.lnorm2(norm_att+drop_att2)\n            prev = norm_att2\n\n        fout = self.ffn(prev)\n        drop_ffn = self.drop3(fout)\n        norm_ffn = self.lnorm3(prev+drop_ffn)\n        \n        return norm_ffn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T08:02:45.671957Z","iopub.execute_input":"2024-12-20T08:02:45.672391Z","iopub.status.idle":"2024-12-20T08:02:45.687521Z","shell.execute_reply.started":"2024-12-20T08:02:45.672355Z","shell.execute_reply":"2024-12-20T08:02:45.685964Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"# Encoder block\nclass Encoder(nn.Module):\n    def __init__(self, n_layers, d_model, d_hidden, n_heads, dk, dv, p_drop):\n        super().__init__()\n        self.layers = nn.ModuleList(\n                [EncoderLayer(d_model, d_hidden, n_heads, dk, dv, p_drop) for _ in range(n_layers)]\n        )\n        self.drop = nn.Dropout(p=p_drop)\n    def forward(self, x, mask):\n        for layer in self.layers:\n            x = layer(x, mask)\n            x = self.drop(x)\n        return x\n        ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Decoder block\nclass Decoder(nn.Module):\n    def __init__(self, n_layers, d_model, d_hidden, n_heads, dk, dv, p_drop):\n        super().__init__()\n        self.layers = nn.ModuleList(\n                [DecoderLayer(d_model, d_hidden, n_heads, dk, dv, p_drop) for _ in range(n_layers)]\n        )\n        self.drop = nn.Dropout(p=p_drop)\n    def forward(self, dec, enc, out_mask, in_mask):\n        x = dec\n        for layer in self.layers:\n            x = layer(x, enc, out_mask, in_mask)\n            x = self.drop(x)\n        return x\n        ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Final Transformer\nclass Tranformer","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}